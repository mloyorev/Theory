{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOo0Iys9Z1HFilKbY0u1d5H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mloyorev/Theory/blob/main/7_OptimalSavingsNumba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install quantecon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huMcySHXpBww",
        "outputId": "21398959-45f3-475b-ad0a-42e468818a22"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: quantecon in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: numba>=0.49.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (0.58.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from quantecon) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.11.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.12)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49.0->quantecon) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->quantecon) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nb1-lA66onao"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import quantecon as qe\n",
        "from numba import njit, prange, int32\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once **again** we use the functions:\n",
        "\n",
        "\n",
        "*   argmax\n",
        "*   succesive_approx\n",
        "\n"
      ],
      "metadata": {
        "id": "lVFvFLz5z2zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@njit\n",
        "def argmax(list_object):\n",
        "    max_val = -np.inf\n",
        "    argmax_index = None\n",
        "    for i, x in enumerate(list_object):\n",
        "        if x > max_val:\n",
        "            max_val = x\n",
        "            argmax_index = i\n",
        "    return argmax_index"
      ],
      "metadata": {
        "id": "O3abe5bDpMTN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def successive_approx(T,\n",
        "                      x_0,\n",
        "                      tolerance=1e-6,\n",
        "                      max_iter=10_000,\n",
        "                      print_step=25,\n",
        "                      verbose=False):\n",
        "    x = x_0\n",
        "    error = tolerance + 1\n",
        "    k = 1\n",
        "    while error > tolerance and k <= max_iter:\n",
        "        x_new = T(x)\n",
        "        error = np.max(np.abs(x_new - x))\n",
        "        if verbose and k % print_step == 0:\n",
        "            print(f\"Completed iteration {k} with error {error}.\")\n",
        "        x = x_new\n",
        "        k += 1\n",
        "    if error > tolerance:\n",
        "        print(f\"Warning: Iteration hit upper bound {max_iter}.\")\n",
        "    elif verbose:\n",
        "        print(f\"Terminated successfully in {k} iterations.\")\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "o08LkhjkpRb_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimal savings model**\n",
        "\n",
        "We assume that **wealth $W_{t}$ evolves** according to\n",
        "\n",
        "$$C_{t}+W_{t+1}≤RW_{t}+Y_{t}$$\n",
        "\n",
        "where $R$ is the gross interest rate, $Y_{t}$ is the labor income and:\n",
        "\n",
        "\n",
        "*   $(W_{t})$ takes values in a finite set $W$ contained in the set $\\mathbb{R}_{+}$\n",
        "*   $(Y_{t})$ is a **$Q$-Markov chain** on a finite and continuous set $Y$.\n",
        "*   $C_{t}\\geq 0$ for all $t$.\n",
        "\n",
        "The individual **aims to maximize the present value** of his utility function. Additionally, we suppose that the individual perceive utility from consumption. This means that the consumer's maximization problem is\n",
        "\n",
        "$$V(w_{t},y_{t})=\\max_{\\left\\{w_{t+\\tau+1}\\right\\}}E_{t}\\sum_{\\tau=0}^{\\infty}\\beta^{\\tau}u(c_{t})=\\max_{\\left\\{w_{t+\\tau+1}\\right\\}}E_{t}\\sum_{\\tau=0}^{\\infty}\\beta^{\\tau}u(Rw_{t}+y_{t}-w_{t+\\tau+1})$$\n",
        "\n",
        "Since $E_{t}u(Rw_{t}+y_{t}-w_{t+1})=u(Rw_{t}+y_{t}-w_{t+1})$, this equation can be also expressed as\n",
        "\n",
        "$$V(w_{t},y_{t})=\\max_{w_{t+1}}u(Rw_{t}+y_{t}-w_{t+1})+\\max_{\\left\\{w_{t+\\tau+1}\\right\\}}E_{t}\\sum_{\\tau=1}^{\\infty}\\beta^{\\tau}u(Rw_{t+\\tau}+y_{t+\\tau}-w_{t+\\tau+1})$$\n",
        "\n",
        "By making the substitution $\\tau=k+1$, we obtain that\n",
        "\n",
        "$$V(w_{t},y_{t})=\\max_{w_{t+1}}u(Rw_{t}+y_{t}-w_{t+1})+\\max_{\\left\\{w_{t+k+2}\\right\\}}E_{t}\\sum_{k=0}^{\\infty}\\beta^{k+1}u(Rw_{t+k+1}+y_{t+k+1}-w_{t+k+2})$$\n",
        "\n",
        "$$V(w_{t},y_{t})=\\max_{w_{t+1}}u(Rw_{t}+y_{t}-w_{t+1})+\\max_{\\left\\{w_{t+k+2}\\right\\}}\\beta E_{t}\\sum_{k=0}^{\\infty}\\beta^{k}u(Rw_{t+k+1}+y_{t+k+1}-w_{t+k+2})$$\n",
        "\n",
        "According to the **law of iterated expectations**$^{1}$ this expression can be rewritten as\n",
        "\n",
        "$$V(w_{t},y_{t})=\\max_{w_{t+1}}u(Rw_{t}+y_{t}-w_{t+1})+\\max_{\\left\\{w_{t+k+2}\\right\\}}\\beta ∫_{y_{t+1}}E_{t+1}\\left[\\sum_{k=0}^{\\infty}\\beta^{k}u(Rw_{t+k+1}+y_{t+k+1}-w_{t+k+2})\\right]f(y_{t+1}|y_{t})dy_{t+1}$$\n",
        "\n",
        "$$V(w_{t},y_{t})=\\max_{w_{t+1}}u(Rw_{t}+y_{t}-w_{t+1})+\\beta ∫_{y_{t+1}}\\max_{\\left\\{w_{t+k+2}\\right\\}}E_{t+1}\\left[\\sum_{k=0}^{\\infty}\\beta^{k}u(Rw_{t+k+1}+y_{t+k+1}-w_{t+k+2})\\right]f(y_{t+1}|y_{t})dy_{t+1}$$\n",
        "\n",
        "$$V(w_{t},y_{t})=\\max_{w_{t+1}}u(Rw_{t}+y_{t}-w_{t+1})+\\beta ∫_{y_{t+1}}V(w_{t+1},y_{t+1})f(y_{t+1}|y_{t})dy_{t+1}$$\n",
        "\n",
        "Finally, **by definition**, we have that\n",
        "\n",
        "$$V(w,y)=\\max_{w'}u(Rw+y-w')+\\beta E_{t}V(w',y')$$\n",
        "\n",
        "which represents the **Bellman equation** for the stochastic dynamic programin problem of the optimal investment problem."
      ],
      "metadata": {
        "id": "NyJoW-imz_sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we define the model we introduce the functions needed to define the default value of the model parameter.\n",
        "\n",
        "Additional to this functions, we suppose that the set $Y$ is discrete, such that the Bellman equation can be written as\n",
        "\n",
        "$$V(w,y)=\\max_{w'}u(Rw+y-w')+\\beta \\sum_{y'\\in Y}V(w',y')Q(y,y')$$\n",
        "\n",
        "where $Q(y,y')$ represents the transition matrix, and that the utility function can be described as\n",
        "\n",
        "$$u(c)=\\frac{c^{1+\\gamma}}{1+\\gamma}$$"
      ],
      "metadata": {
        "id": "9hoPu4SkK6jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model = namedtuple('Model', ('beta',    # Discount factor\n",
        "                             'R',       # Gross interest rate\n",
        "                             'gamma',   # CRRA parameter\n",
        "                             'w_grid',  # Wealth grid\n",
        "                             'y_grid',  # Labor income grid\n",
        "                             'Q'))      # Labor income transition matrix"
      ],
      "metadata": {
        "id": "6kdtw7O-pSyT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_consumption_model(R=1.01,         # Gross interest rate\n",
        "                             beta=0.98,      # Discount factor\n",
        "                             gamma=2.5,      # CRRA parameter\n",
        "                             w_min=0.01,     # Min wealth\n",
        "                             w_max=5.0,      # Max wealth\n",
        "                             w_size=150,     # Size of the wealth grid\n",
        "                             rho=0.9, nu=0.1,   # Income parameters\n",
        "                             y_size=100):    # Size of the income grid\n",
        "\n",
        "    w_grid = np.linspace(w_min, w_max, w_size)   # Create the wealth grid\n",
        "    mc = qe.tauchen(rho=rho, sigma=nu, n=y_size) # 'qe.tauchen' computes a Markov chain associated with a discretized version of the linear Gaussian AR(1) process\n",
        "                                                 #    - rho is the autocorrelation coefficient\n",
        "                                                 #    - nu is the standard deviation of the random process\n",
        "    y_grid, Q = np.exp(mc.state_values), mc.P    # - 'y_grid' is the grid of transformed income values\n",
        "                                                 # - 'Q' is the transition matrix describing transition probabilities between discrete income values ​​in the stochastic process\n",
        "\n",
        "    return Model(beta=beta, R=R, gamma=gamma, w_grid=w_grid, y_grid=y_grid, Q=Q)"
      ],
      "metadata": {
        "id": "pUSePoYgqEla"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the RHS of the unmaximized Bellman equation"
      ],
      "metadata": {
        "id": "By8lleXRMJC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@njit\n",
        "def B(i, j, ip, v, model):\n",
        "\n",
        "    β, R, γ, w_grid, y_grid, Q = model          # Unpacked model parameters\n",
        "    w, y, wp = w_grid[i], y_grid[j], w_grid[ip] # We use:\n",
        "                                                #  - index i for access the value of current wealth within the wealth grid\n",
        "                                                #  - index j for access the value of income within the income grid\n",
        "                                                #  - index ip foe acces the value of next period wealth within the wealth grid\n",
        "    c = R * w + y - wp\n",
        "    if c > 0:\n",
        "        return c**(1 - γ) / (1 - γ) + β * np.dot(v[ip, :], Q[j, :])\n",
        "    return - np.inf"
      ],
      "metadata": {
        "id": "1nSHbiZ-tE_z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to solve this model, we have to prove that it satisfies the **Blackwell sufficiency conditions**.\n",
        "\n",
        "### **1. Monotonicity**\n",
        "Let $V,W$ two different functions that satisfy that $V(w,y)≤W(w,y)\n",
        "$ $∀w,y$. If we name $w',y'$ the wealth and the labor income shock of the next period then...\n",
        "\n",
        "$$V(w',y')≤W(w',y'),  ∀w',y'$$\n",
        "\n",
        "If we multiply both sides of the inequality by the transition probability matrix, then...\n",
        "\n",
        "$$V(w',y')Q(y,y')≤W(w',y')Q(y,y'),  ∀w',y'$$\n",
        "\n",
        "The inequality also holds for the expected value of value functions, such that...\n",
        "\n",
        "$$\\sum_{y'}V(w',y')Q(y,y')≤\\sum_{y'}W(w',y')Q(y,y')$$\n",
        "$$β\\sum_{y'}V(w',y')Q(y,y')≤β\\sum_{y'}W(w',y')Q(y,y')$$\n",
        "\n",
        "Finally we add both sides the maximization of the current utility given a particular level of current wealth and income shock, such that...\n",
        "\n",
        "$$\\max_{w'}u(w,y,w')+β\\sum_{y'}V(w',y')Q(y,y')≤\\max_{w'}u(w,y,w')+β\\sum_{y'}W(w',y')Q(y,y')$$\n",
        "\n",
        "Therefore, $T(V(w,y))≤T(W(w,y)), ∀w,y$, which means that the transformation **satisifes monotonicity**.\n",
        "\n",
        "### **2. Discounting**\n",
        "Let's consider a value function $V$ and a positive constant $a$, such that...\n",
        "\n",
        "$$T(V(w,y)+a)=\\max_{w'}u(w,y,w')+β∑_{y'}(V(w',y')+a)Q(y,y')$$\n",
        "\n",
        "\n",
        "\n",
        "$$T(V(w,y)+a)=\\max_{w'}u(w,y,w')+β∑_{y'}(V(w',y'))Q(y,y')+β∑_{y'}aQ(y,y')$$\n",
        "\n",
        "\n",
        "\n",
        "$$T(V(w,y)+a)=\\max_{w'}u(w,y,w')+β∑_{y'}(V(w',y'))Q(y,y')+aβ∑_{y'}Q(y,y')$$\n",
        "\n",
        "it is fulfilled that $∑_{y'}Q(y,y')=1$, such that...\n",
        "\n",
        "$$T(V(w,y)+a)=\\max_{w'}u(w,y,w')+β∑_{y'}(V(w',y'))Q(y,y')+aβ$$\n",
        "\n",
        "\n",
        "\n",
        "$$T(V(w,y)+a)=T(V(w,y))+aβ$$\n",
        "\n",
        "Therefore, $T(V(w,y)+a)≤T(V(w,y))+aβ$, which means that the transformation satisfies discounting.\n"
      ],
      "metadata": {
        "id": "mFcpEli4MstQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we already know that the model satisfies the Blackwell sufficiency conditions, then we are now sure that **there is a solution and that it is unique**.\n",
        "\n",
        "Unlike previous notebooks, we will now solve the model with **three different algorithms**:\n",
        "\n",
        "\n",
        "*   Value Function Iteration (VFI).\n",
        "*   Howard Policy Iteration (HPI).\n",
        "*   Optimistic Policy Iteration (OPI).\n",
        "\n"
      ],
      "metadata": {
        "id": "EPLI3NNUN1kF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to be able to use this algorithms, we have to define the policy operator $T_{\\sigma}$, the Bellman operator $T$ and the $v-$greedy policy given $v$."
      ],
      "metadata": {
        "id": "DYBrVPlMMUdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POLICY OPERATOR\n",
        "\n",
        "@njit(parallel=True)          # It is compiled with numba and the inner loop can be parallelized with multiple CPU cores\n",
        "def T_sigma(v, sigma, model):\n",
        "\n",
        "    w_size, y_size = len(model.w_grid), len(model.y_grid) # Obtain the size of the wealth and income grids\n",
        "    v_new = np.empty_like(v)                              # Initialize a value fuction of the size of the 'v' array\n",
        "\n",
        "    for i in prange(w_size):                              # Iterates through the i indices of the wealth grid\n",
        "                                                          # 'prange(w_size)' indicates that the loop can be paralelized\n",
        "        for j in range(y_size):                           # Iterates through the j indices of the income grid\n",
        "            v_new[i, j] = B(i, j, sigma[i, j], v, model)  # It gets the value of each possible combination of wealth and income\n",
        "                                                          # Notiche that:\n",
        "                                                          #   - i is the index of current wealth\n",
        "                                                          #   - j is the index of the current income\n",
        "                                                          #   - 'sigma[i, j]' is the next period wealth given the indices of current wealth and income\n",
        "    return v_new"
      ],
      "metadata": {
        "id": "RKJKTZ4aPuTx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BELLMAN OPERATOR\n",
        "\n",
        "@njit(parallel=True)\n",
        "def T(v, model):\n",
        "\n",
        "    β, R, γ, w_grid, y_grid, Q = model\n",
        "    w_size, y_size = len(w_grid), len(y_grid)\n",
        "    v_new = np.empty_like(v)\n",
        "\n",
        "    for i in prange(w_size):\n",
        "        for j in range(y_size):\n",
        "            v_new[i, j] = max([B(i, j, ip, v, model) for ip in range(w_size)])  # It gets the value of each possible combination of wealth and income\n",
        "                                                                                # Notiche that:\n",
        "                                                                                #   - i is the index of current wealth\n",
        "                                                                                #   - j is the index of the current income\n",
        "                                                                                #   - ip is the next period wealth\n",
        "    return v_new"
      ],
      "metadata": {
        "id": "YKvCX2LUR3Xd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V-GREEDY POLICY GIVEN V\n",
        "\n",
        "@njit(parallel=True)\n",
        "def get_greedy(v, model):\n",
        "\n",
        "    β, R, γ, w_grid, y_grid, Q = model\n",
        "    w_size, y_size = len(w_grid), len(y_grid)\n",
        "    σ = np.empty_like(v, dtype=int32)\n",
        "\n",
        "    for i in prange(w_size):\n",
        "        for j in range(y_size):\n",
        "            σ[i, j] = argmax([B(i, j, ip, v, model) for ip in range(w_size)]) #   For each pair (i,j), we obtain the index ip that maximize\n",
        "                                                                              # the Bellman equation.\n",
        "    return σ"
      ],
      "metadata": {
        "id": "Yq6luUUmSihy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function below computes the value $v_{\\sigma}$ of following a policy $\\sigma$.\n",
        "\n",
        "In order to compute the value of a policy, we have to solve the linear system\n",
        "\n",
        "$$v(w,y)=u(Rw+y-\\sigma(w,y))+β\\sum_{y'}v(\\sigma(w,y),y')Q(y',y)$$\n",
        "\n",
        "for $v$.\n",
        "\n",
        "Therefore, it turns out helpul to rewrite this system as\n",
        "\n",
        "$$v(w,y)=r(w,y,\\sigma(w,y))+β\\sum_{w',y'}v(w',y')P_{\\sigma}(w,y,w',y')$$\n",
        "\n",
        "where $P_{\\sigma}(w,y,w',y')=1\\left\\{w'=\\sigma(w,y)\\right\\}Q(y',y)$\n",
        "\n",
        "Thus the linear system can be simplied as\n",
        "\n",
        "$$v=r_{\\sigma}+P_{\\sigma}Q(y',y)$$\n",
        "\n",
        "and the solve for $v$.\n",
        "\n",
        "Notice that:\n",
        "\n",
        "*   $v$ is a $2$ index array.\n",
        "*   $P_{\\sigma}$ is a $4$ index array.\n",
        "\n",
        "Then, the code below:\n",
        "\n",
        "1.   Reshapes $v$ and $r_{\\sigma}$ to 1D arrays and $P_{\\sigma}$ to a matrix.\n",
        "2.   Solves the linear system.\n",
        "3.   Converts back to multi-index arrays.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oAgl37ZxTaTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GET THE VALUE V_SIGMA OF POLICY SIGMA\n",
        "\n",
        "@njit(parallel=True)\n",
        "def get_value(sigma, model):\n",
        "\n",
        "    # Unpack and set up\n",
        "    beta, R, gamma, w_grid, y_grid, Q = model\n",
        "    nw, ny = len(w_grid), len(y_grid)\n",
        "    n = nw * ny\n",
        "\n",
        "    # Allocate and create multi-index versions of P_σ and r_σ\n",
        "    P_sigma = np.zeros((nw, ny, nw, ny)) # Initialize the array P_sigma with 4 dimensions\n",
        "    r_sigma = np.zeros((nw, ny))         # Initialize the array r_simga with 2 dimensions\n",
        "    for i in range(nw):\n",
        "        for j in range(ny):\n",
        "            w, y, wp = w_grid[i], y_grid[j], w_grid[sigma[i, j]]  # Get into each position of wealth and income\n",
        "            c = R * w + y - wp                                    # Obtain the feasible level of consumption\n",
        "            r_sigma[i, j] = c**(1 - gamma) / (1 - gamma)          # Get the utility level of each feaible level of consumotion\n",
        "            for ip in range(nw):\n",
        "                for jp in range(ny):\n",
        "                    if ip == sigma[i, j]:                         # If w'=sigma(w,y), then  P_sigma[i, j, ip, jp] = Q[j, jp]\n",
        "                        P_sigma[i, j, ip, jp] = Q[j, jp]\n",
        "\n",
        "    # Reshape to standard matrix algebra form\n",
        "    r_sigma = r_sigma.reshape((n,))\n",
        "    P_sigma = P_sigma.reshape((n, n))\n",
        "\n",
        "    # Solve for the value of σ\n",
        "    I = np.identity(n)\n",
        "    v_sigma = np.linalg.solve(I - beta * P_sigma, r_sigma)   # Solves the lineas system given by [I - beta * P_sigma]v_sigma=r_sigma\n",
        "\n",
        "    # Return as multi-index array\n",
        "    return np.reshape(v_sigma, (nw, ny))"
      ],
      "metadata": {
        "id": "VTEZuPJOVS8t"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are able to define the solvers for each algorithm.\n",
        "\n",
        "### **1. Value Function Iteration (VFI)**\n",
        "\n",
        "The VFI algorithm have already been described previously in the `2_NeoclasicalOptimalGrowthModel.ipynb` notebook.\n",
        "\n",
        "We introduce a function that implements VFI."
      ],
      "metadata": {
        "id": "-C88Kx5_ZXZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(model, tol=1e-5):\n",
        "  v0 = np.zeros((len(model.w_grid), len(model.y_grid)))                  # Initial guess\n",
        "  v_star = successive_approx(lambda v: T(v, model), v0, tolerance = tol) # Succesive approximation algorithm\n",
        "  return get_greedy(v_star, model)                                       # Returns the optimal policy"
      ],
      "metadata": {
        "id": "Fp-UOTg3aBsd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Howard Policy Iteration (HPI)**\n",
        "\n",
        "The HPI algorithm may be **described as follows:**\n",
        "\n",
        "\n",
        "1.   Set an initial feasible set of decision rule for the control variable $y=f_{0}(x,s_{k}), k=0,1,...,M$.\n",
        "2.   Compute the value of $y=f_{0}(x,s_{k})$, assuming that this rule is operative forever, taking care of the fact that $x_{t+1}=h(x_{t},f_{i}(x_{t},s_{t}),s_{t})$ with $i_0$.\n",
        "3.   Set a stopping criterion $ε>0$.\n",
        "4.   Find a new policy rule $y=f_{i+1}(x,s_{k})$, such that\n",
        "\n",
        "$$f_{i+1}(x,s_{k})\\in\\text{arg}\\max_{y}u(y,x,s_{k})+\\beta \\sum_{j=1}^{M}\\pi_{kj}V(h(x_{t},f_{i}(x_{t},s_{t}),s_{t}),s'_{j})$$\n",
        "5.   Check if $||f_{i+1}(x,s)-f_{i}(x,s)||<ɛ$, if yes then stop, else go back to step 2.\n",
        "\n",
        "Notice that this methos **differs fundamentally from the VFI** algorithm in at least two dimensions:\n",
        "\n",
        "\n",
        "1.   Iterates on the policy function rather than on the value function.\n",
        "2.   The decision rule is used forever whereas it is assumed inly two consecutive periods in the VFI algorithm. This is precisely what accelerates convergence.\n",
        "\n",
        "As we mentioned above, note that when computing the value function we actually have to solve a linear system of the form\n",
        "\n",
        "$$v(w,y)=u(Rw+y-\\sigma(w,y))+β\\sum_{y'}v(\\sigma(w,y),y')Q(y',y)$$\n",
        "\n",
        "for $v$.\n",
        "\n",
        "Usually HPI requieres only few iterations. Unfortunately, we have to solve a relatively difficult linear system, which **may be particularly costly** when the number of grid points is important.\n",
        "\n",
        "Therefore, a number of researchers has proposed to replace the matrix inversion by an additional iteration step, leading to the so-called **Optimistic Policy Iteration (OPI)**.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wezI4T_4bEub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(model):\n",
        "    w_size, y_size = len(model.w_grid), len(model.y_grid)\n",
        "    sigma = np.zeros((w_size, y_size), dtype=int)             # Initial guess\n",
        "    i, error = 0, 1.0                                         # Initialize iterations and error\n",
        "    while error > 0:\n",
        "        v_sigma = get_value(sigma, model)                # Get the value of the policy guess\n",
        "        sigma_new = get_greedy(v_sigma, model)           # Updates the policy given the value of the previous policy\n",
        "        error = np.max(np.abs(sigma_new - sigma))        # Updates error\n",
        "        sigma = sigma_new                                # Updates policy\n",
        "        i = i + 1                                        # Updates number of iterations\n",
        "        print(f\"Concluded loop {i} with error {error}.\")\n",
        "\n",
        "    return sigma"
      ],
      "metadata": {
        "id": "Kwr5xxOsbaJE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Optimistical Policy Iteration (OPI)**\n",
        "\n",
        "The OPI algorithm may be **described as follows:**\n",
        "\n",
        "1.   Set J_{0}=V_{i}.\n",
        "2.   Iterates $m\\in\\mathbb{N}$ times on\n",
        "\n",
        "$$J_{i+1}=u(y,x)+\\beta QJ_{i}$$\n",
        "\n",
        "3.   Set $V_{i+1}=J_{k}$ and compute the error $||V_{i+1}-V_{i}||$.\n",
        "4.   If the error is lower than the convergence criterion, then we have arrive to the solution, otherwise return to the step 2.\n",
        "\n",
        "**Regarding $m$,**\n",
        "\n",
        "\n",
        "*   $m→\\infty⇒OPI=HPI$\n",
        "*   $m→1⇒OPI=VFI$\n",
        "\n",
        "Often an intermediate value of $m$ is better than both.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vh-zMwH6v3RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimistic_policy_iteration(model, tol=1e-5, m=100):\n",
        "  v = np.zeros((len(model.w_grid), len(model.y_grid))) # Initial guess of the value function\n",
        "  error = tol + 1                                      # Initialize the error\n",
        "\n",
        "  while error > tol:\n",
        "      last_v = v\n",
        "      sigma = get_greedy(v, model)          # Get the policy associated with the guess of the value function\n",
        "      for _ in range(m):                    # Apply the policy operator m times\n",
        "        v = T_sigma(v, sigma, model)        # Update the value function\n",
        "        error = np.max(np.abs(v - last_v))  # Update the error\n",
        "  return get_greedy(v, model)"
      ],
      "metadata": {
        "id": "f3kpFjbUwan8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a quick test of the timing of each algorithm."
      ],
      "metadata": {
        "id": "4jj0i9Av5zae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_consumption_model()"
      ],
      "metadata": {
        "id": "xBblBxkp53pi"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}