{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPdIGZXrWZlykytIJwEaKc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mloyorev/Theory/blob/main/9_OptimalSavingsJax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9a4GwS4jAYH",
        "outputId": "480aa5c5-b134-43ce-a72c-48cafd018b90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting quantecon\n",
            "  Downloading quantecon-0.7.1-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.8/214.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.49.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (0.58.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from quantecon) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.11.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.12)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49.0->quantecon) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->quantecon) (1.3.0)\n",
            "Installing collected packages: quantecon\n",
            "Successfully installed quantecon-0.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install quantecon\n",
        "import quantecon as qe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import quantecon as qe\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vIhE346djLFb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One difference between `numpy` and `jax` is that, when running on a GPU, `jax` uses 32 bit floats by default.  This is standard for GPU computing and can lead to significant speed gains with small loss of precision.\n",
        "\n",
        "However, **for some calculations precision matters**. In this case we will use 64 bit floats with `jax`  in order to match with `7_OptimalSavingsNumba.ipynb` (this is because `numpy` use double precision floating point numbers, also known as `float64`)."
      ],
      "metadata": {
        "id": "IBP5RZ8kjXDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jax.config.update(\"jax_enable_x64\", True)"
      ],
      "metadata": {
        "id": "hG4qTLBGkOe2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in the previuous notebooks, we will define the following functions:\n",
        "\n",
        "\n",
        "*   `supremum_norm`\n",
        "*   `succesive_approx`\n",
        "\n"
      ],
      "metadata": {
        "id": "fDgUPU0vkYkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def supremum_norm(x,x_new):\n",
        "  return np.max(jnp.abs(x_new - x))\n",
        "\n",
        "def successive_approx(T,\n",
        "                      x_0,\n",
        "                      M,\n",
        "                      tolerance=1e-6,\n",
        "                      max_iter=10_000,\n",
        "                      print_step=25,\n",
        "                      verbose=False):\n",
        "\n",
        "    x = x_0\n",
        "    error = tolerance + 1\n",
        "    k = 1\n",
        "\n",
        "    while error > tolerance and k <= max_iter:\n",
        "        x_new = T(x)\n",
        "        error = M(x, x_new)\n",
        "        if verbose and k % print_step == 0:\n",
        "            print(f\"Completed iteration {k} with error {error}.\")\n",
        "        x = x_new\n",
        "        k += 1\n",
        "\n",
        "    if error > tolerance:\n",
        "        print(f\"Warning: Iteration hit upper bound {max_iter}.\")\n",
        "\n",
        "    elif verbose:\n",
        "        print(f\"Terminated successfully in {k} iterations.\")\n",
        "    return x"
      ],
      "metadata": {
        "id": "mZV-18q7kx98"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimal savings model with Google Jax**\n",
        "\n",
        "As we have alredy mentioned, the **Bellman equation** of this model is given by\n",
        "\n",
        "$$V(w,y)=\\max_{w'}u(Rw+y-w')+\\beta\\sum_{y'\\in Y}V(w',y')Q(y',y)$$\n",
        "\n",
        "where\n",
        "\n",
        "$$u(c)=\\frac{c^{1+\\gamma}}{1+\\gamma}$$\n",
        "\n",
        "**Details on the model assumptions and detailed derivation of the Bellman equation** comes in the `7_OptimalSavingsNumba` notebook.\n",
        "\n",
        "The main objetive of this notebook is to prove **how `jax` improve the performance** of the model solvers that we create in `7_OptimalSavingsNumba`"
      ],
      "metadata": {
        "id": "gh0p5stFlEEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model = namedtuple('Model',('beta', 'R', 'gamma', 'w_grid', 'y_grid', 'Q'))\n",
        "\n",
        "def create_consumption_model(R=1.01,                    # Gross interest rate\n",
        "                             beta=0.98,                 # Discount factor\n",
        "                             gamma=2.5,                 # CRRA parameter\n",
        "                             w_min=0.01,                # Min wealth\n",
        "                             w_max=5.0,                 # Max wealth\n",
        "                             w_size=150,                # Grid side\n",
        "                             rho=0.9,                   # Persistance of the shock\n",
        "                             nu=0.1,                    # Volatility of stochastic error (sigma)\n",
        "                             y_size=100):\n",
        "\n",
        "    w_grid = np.linspace(w_min, w_max, w_size)\n",
        "    mc = qe.tauchen(rho= rho, sigma = nu, n = y_size)\n",
        "    y_grid, Q = np.exp(mc.state_values), mc.P\n",
        "    return Model(beta=beta, R=R, gamma=gamma, w_grid=w_grid, y_grid=y_grid, Q=Q)"
      ],
      "metadata": {
        "id": "RZP0fDATmm-3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use `jax` **we have to create a `jax`-compatible version** of the consumption model.\n",
        "\n",
        "This `jax`-compatible version of the consumption model:\n",
        "\n",
        "\n",
        "*  Break up parameters into static and nonstatic components\n",
        "*   Move the arrays to a device, for example the GPU or the TPU.\n",
        "\n",
        "Moving the arrays to a device is very **important because:**\n",
        "\n",
        "\n",
        "1.   GPUs (Graphics Processing Unit) and TPUs (Tensor Processing Unit) are designed to **perform parallel calculations** much faster than CPUs (Central Processing Unit).\n",
        "2.   By moving data to the device, JAX is allowed to perform **parallel calculations** on specialized hardware, which can significantly speed up calculation performance and efficiency.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JyNQzXzgo3Dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a JAX-compatible version of the consumption model\n",
        "def create_consumption_model_jax():\n",
        "\n",
        "    model = create_consumption_model()          # Create an instance of the model\n",
        "    beta, R, gamma, w_grid, y_grid, Q = model   # Unpacked model parameters\n",
        "\n",
        "    # Break up parameters into static and nonstatic components\n",
        "    constants = beta, R, gamma\n",
        "    sizes = len(w_grid), len(y_grid)\n",
        "    arrays = w_grid, y_grid, Q\n",
        "\n",
        "    arrays = tuple(map(jax.device_put, arrays)) # Shift arrays to the device (e.g., GPU and TPU)\n",
        "\n",
        "    return constants, sizes, arrays"
      ],
      "metadata": {
        "id": "OYABl6S9oZI5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we compute the RHS of the **Bellman equation**."
      ],
      "metadata": {
        "id": "SjAGxQUXswBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def B(v, constants, sizes, arrays):\n",
        "\n",
        "    # Unpack model parameters\n",
        "    β, R, γ = constants\n",
        "    w_size, y_size = sizes\n",
        "    w_grid, y_grid, Q = arrays\n",
        "\n",
        "    # Notice that index:\n",
        "    #   - i is for current wealth\n",
        "    #   - j is for current income\n",
        "    #   - ip is for next period's wealth\n",
        "    #   - jp is for next period's income\n",
        "\n",
        "    # Compute current rewards r(w, y, wp) as an array r[i, j, ip] of three dimesions\n",
        "    w  = jnp.reshape(w_grid, (w_size, 1, 1))    # w[i]   ->  w[i, j, ip]\n",
        "    y  = jnp.reshape(y_grid, (1, y_size, 1))    # z[j]   ->  z[i, j, ip]\n",
        "    wp = jnp.reshape(w_grid, (1, 1, w_size))    # wp[ip] -> wp[i, j, ip]\n",
        "    c = R * w + y - wp                          # Current reward\n",
        "\n",
        "    # Calculate continuation rewards at all combinations of (w, y, wp)\n",
        "    v = jnp.reshape(v, (1, 1, w_size, y_size))  # v[ip, jp] -> v[i, j, ip, jp]\n",
        "    Q = jnp.reshape(Q, (1, y_size, 1, y_size))  # Q[j, jp]  -> Q[i, j, ip, jp]\n",
        "    EV = jnp.sum(v * Q, axis=3)                 # Sum over last index jp for the expected value\n",
        "\n",
        "    # Comoute the RHS of the Bellman equation if and only if c>0, otherwise set -np.inf\n",
        "    return jnp.where(c > 0, c**(1-γ)/(1-γ) + β * EV, -np.inf)"
      ],
      "metadata": {
        "id": "_IxPuSYFs1P6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we need to ***prove that our Bellman equation satisfies the Blackwell's sufficiency conditions*** to be a contraction mapping .\n",
        "\n",
        "For a detailed demonstration of Blackwell's sufficiency conditions for this case, check the `7_OptimalSavingsNumba.ipynb` notebook."
      ],
      "metadata": {
        "id": "B8WLJZilvI2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we **define the necessary operators** for creating the solvers of the model:\n",
        "\n",
        "\n",
        "*   **`COMPUTE CURRENT REWARD`:** Compute the array $r_\\sigma(w, y) = r(w,y, σ(w, y))$, which gives current rewards given policy sigma.\n",
        "*   **`BELLMAN OPERATOR`:** Compute the maximization of the Bellman equation along next period's wealth.\n",
        "*   **`GET GREEDY`:** Compute the index of the argument that maximizr the Bellman equation along the next period's wealth.\n",
        "*   **`POLICY OPERATOR`:** Compute the value associated with a particular policy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rhwenvPHvTti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----COMPUTE CURRENT REWARD-----\n",
        "def compute_r_sigma(sigma, constants, sizes, arrays):\n",
        "  # Unpack model\n",
        "    beta, R, gamma = constants\n",
        "    w_size, y_size = sizes\n",
        "    w_grid, y_grid, Q = arrays\n",
        "\n",
        "  # Compute r_σ[i, j]\n",
        "    w = jnp.reshape(w_grid, (w_size, 1))  # w[i]   ->  w[i, j]\n",
        "    y = jnp.reshape(y_grid, (1, y_size))  # y[j]   ->  y[i, j]\n",
        "    wp = w_grid[sigma]                    # Selection of values ​​from a grid w_grid using indices contained in an array called sigma.\n",
        "    c = R * w + y - wp                    # Compute consumption\n",
        "    r_sigma = c**(1-gamma)/(1-gamma)      # Compute current reward\n",
        "\n",
        "    return r_sigma\n",
        "\n",
        "# -----BELLMAN OPERATOR-----\n",
        "def T(v, constants, sizes, arrays):\n",
        "    return jnp.max(B(v, constants, sizes, arrays), axis=2)  #   The result of the B function is passed through the jnp.max function with axis=2. This means that\n",
        "                                                            # the maximum value is computed along the third axis, which corresponds to the 'wp' axis. This operation\n",
        "                                                            # calculates the maximum value for each combination of (w,y). The result of the T function is a 2D matrix\n",
        "\n",
        "# -----GET GREEDY-----\n",
        "def get_greedy(v, constants, sizes, arrays):\n",
        "    return jnp.argmax(B(v, constants, sizes, arrays), axis=2) #   The result of the B function is then passed through the jnp.argmax function with axis=2. This means that the index\n",
        "                                                              # of the maximum value is computed along the third axis, which corresponds to the 'wp' axis. This operation finds the\n",
        "                                                              # index of the action that maximizes RHS of the Bellman equation for each combination (w,y).\n",
        "\n",
        "# -----POLICY OPERATOR-----\n",
        "def T_sigma(v, sigma, constants, sizes, arrays):\n",
        "    # Unpack model\n",
        "    beta, R, gamma = constants\n",
        "    w_size, y_size = sizes\n",
        "    w_grid, y_grid, Q = arrays\n",
        "\n",
        "    r_sigma = compute_r_sigma(sigma, constants, sizes, arrays)  # Compute current reward\n",
        "\n",
        "    # Compute the array v[σ[i, j], jp]\n",
        "    yp_idx = jnp.arange(y_size)\n",
        "    yp_idx = jnp.reshape(yp_idx, (1, 1, y_size))\n",
        "    sigma = jnp.reshape(sigma, (w_size, y_size, 1))\n",
        "    V = v[sigma, yp_idx]\n",
        "\n",
        "    # Convert Q[j, jp] to Q[i, j, jp]\n",
        "    Q = jnp.reshape(Q, (1, y_size, y_size))\n",
        "\n",
        "    return r_sigma + beta * np.sum(V * Q, axis=2)"
      ],
      "metadata": {
        "id": "2Ow-Lyfov9ic"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functions below **compute the value $v_{\\sigma}$ of following a policy $\\sigma$**.\n",
        "\n",
        "This lifetime value is a function $v_{\\sigma}$ that satisifies\n",
        "\n",
        "$$v_{\\sigma}(w,y)=r_{\\sigma}(w,y)+\\beta\\sum_{y'}v_{\\sigma}(\\sigma(w,y),y')Q(y',y)$$\n",
        "\n",
        "where we want to solve for $v_{\\sigma}$."
      ],
      "metadata": {
        "id": "XtNtMxH2USD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we define the linear operator $L_{\\sigma}$ by\n",
        "\n",
        "$$(L_{\\sigma}v)(w,y)=v(w,y)-\\beta\\sum_{y'}v_(\\sigma(w,y),y')Q(y',y)$$\n",
        "\n",
        "With this notation, the objective is to solve for $v$ via\n",
        "\n",
        "$$(L_{\\sigma}v)(w,y)=r_{\\sigma}(w,y)$$\n",
        "\n",
        "which in vector notation is $L_{\\sigma}v=r_{\\sigma}$. The vector notation of the problem implies that\n",
        "\n",
        "$$v=L_{\\sigma}^{-1}r_{\\sigma}$$"
      ],
      "metadata": {
        "id": "R6unYKA9U2rM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`jax` allows us to solve linear systes defined in terms of operators. The first step is to define the function $L_{\\sigma}$."
      ],
      "metadata": {
        "id": "4bFodiPiV6kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L_sigma(v, sigma, constants, sizes, arrays):\n",
        "    # Unpack model\n",
        "    beta, R, gamma = constants\n",
        "    w_size, y_size = sizes\n",
        "    w_grid, y_grid, Q = arrays\n",
        "\n",
        "    # Set up the array v[σ[i, j], jp]\n",
        "    zp_idx = jnp.arange(y_size)                     # Create one-dimensional arrays ranging from 0 to y_size-1\n",
        "    zp_idx = jnp.reshape(zp_idx, (1, 1, y_size))    # Reshape zp_idx as an array of three dimensiones (i,j,ip)\n",
        "    sigma = jnp.reshape(sigma, (w_size, y_size, 1)) # Reshape sigma as an array of three dimensiones (i,j,ip)\n",
        "    V = v[sigma, zp_idx]                            # Evaluate v(w',y')\n",
        "\n",
        "    # Expand Q[j, jp] to Q[i, j, jp]\n",
        "    Q = jnp.reshape(Q, (1, y_size, y_size))\n",
        "\n",
        "    # Compute and return v[i, j] - β Σ_jp v[σ[i, j], jp] * Q[j, jp]\n",
        "    return v - beta * np.sum(V * Q, axis=2)"
      ],
      "metadata": {
        "id": "juAh7cPpWJpo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define a function that computes $v_{\\sigma}$ with the help of the previous function."
      ],
      "metadata": {
        "id": "l_Gux_FLXwSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_value(sigma, constants, sizes, arrays):\n",
        "    # Unpack\n",
        "    beta, R, gamma = constants\n",
        "    w_size, y_size = sizes\n",
        "    w_grid, y_grid, Q = arrays\n",
        "\n",
        "    r_sigma = compute_r_sigma(sigma, constants, sizes, arrays)               # Computes current reward\n",
        "\n",
        "    # Reduce R_σ to a function in v\n",
        "    partial_R_sigma = lambda v: L_sigma(v, sigma, constants, sizes, arrays)  # Defines a function 'partial_R_sigma' that takes v as an argument\n",
        "\n",
        "    return jax.scipy.sparse.linalg.bicgstab(partial_R_sigma, r_sigma)[0] #  Uses the BiCGSTAB iterative method to solve a system of linear equations of the form\n",
        "                                                                         # (partial_R_sigma)x = r_sigma.BiCGSTAB is an iterative method that, in each iteration,\n",
        "                                                                         #  calculates an approximation to the solution x of the system of equations."
      ],
      "metadata": {
        "id": "ZRKriR5qXu9W"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the ***JIT compiled versions*** of the operators. A JIT compiled version refers to a version of a program that has been compiled using a technique called just-in-time (JIT) compilation.\n",
        "\n",
        "\n",
        "In this case the functions defined earlier are compiled using `jax.jit` and checking `static_argnums`. Where `static_argnums` indicates the function argument that remains static, particularly."
      ],
      "metadata": {
        "id": "Vpr_g3ByaBXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The static argument is 'sizes'\n",
        "B = jax.jit(B, static_argnums=(2,))\n",
        "compute_u_σ = jax.jit(compute_r_sigma, static_argnums=(2,))\n",
        "T = jax.jit(T, static_argnums=(2,))\n",
        "get_greedy = jax.jit(get_greedy, static_argnums=(2,))\n",
        "get_value = jax.jit(get_value, static_argnums=(2,))\n",
        "\n",
        "# The static argument is 'sizes'\n",
        "T_sigma = jax.jit(T_sigma, static_argnums=(3,))\n",
        "L_sigma = jax.jit(L_sigma, static_argnums=(3,))"
      ],
      "metadata": {
        "id": "uj6B6EK1Zjcn"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}