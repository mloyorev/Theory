{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6e3nmXzpmhMik6W3ukRbJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mloyorev/Theory/blob/main/9_OptimalSavingsJax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9a4GwS4jAYH",
        "outputId": "cc30f4a8-9f83-47a7-b2bd-33d0b41ce567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting quantecon\n",
            "  Downloading quantecon-0.7.1-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.8/214.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.49.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (0.58.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from quantecon) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.11.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.12)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49.0->quantecon) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->quantecon) (1.3.0)\n",
            "Installing collected packages: quantecon\n",
            "Successfully installed quantecon-0.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install quantecon\n",
        "import quantecon as qe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import quantecon as qe\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vIhE346djLFb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One difference between `numpy` and `jax` is that, when running on a GPU, `jax` uses 32 bit floats by default.  This is standard for GPU computing and can lead to significant speed gains with small loss of precision.\n",
        "\n",
        "However, **for some calculations precision matters**. In this case we will use 64 bit floats with `jax`  in order to match with `7_OptimalSavingsNumba.ipynb` (this is because `numpy` use double precision floating point numbers, also known as `float64`)."
      ],
      "metadata": {
        "id": "IBP5RZ8kjXDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jax.config.update(\"jax_enable_x64\", True)"
      ],
      "metadata": {
        "id": "hG4qTLBGkOe2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in the previuous notebooks, we will define the following functions:\n",
        "\n",
        "\n",
        "*   `supremum_norm`\n",
        "*   `succesive_approx`\n",
        "\n"
      ],
      "metadata": {
        "id": "fDgUPU0vkYkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def supremum_norm(x,x_new):\n",
        "  return np.max(jnp.abs(x_new - x))\n",
        "\n",
        "def successive_approx(T,\n",
        "                      x_0,\n",
        "                      M,\n",
        "                      tolerance=1e-6,\n",
        "                      max_iter=10_000,\n",
        "                      print_step=25,\n",
        "                      verbose=False):\n",
        "\n",
        "    x = x_0\n",
        "    error = tolerance + 1\n",
        "    k = 1\n",
        "\n",
        "    while error > tolerance and k <= max_iter:\n",
        "        x_new = T(x)\n",
        "        error = M(x, x_new)\n",
        "        if verbose and k % print_step == 0:\n",
        "            print(f\"Completed iteration {k} with error {error}.\")\n",
        "        x = x_new\n",
        "        k += 1\n",
        "\n",
        "    if error > tolerance:\n",
        "        print(f\"Warning: Iteration hit upper bound {max_iter}.\")\n",
        "\n",
        "    elif verbose:\n",
        "        print(f\"Terminated successfully in {k} iterations.\")\n",
        "    return x"
      ],
      "metadata": {
        "id": "mZV-18q7kx98"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimal savings model with Google Jax**\n",
        "\n",
        "As we have alredy mentioned, the **Bellman equation** of this model is given by\n",
        "\n",
        "$$V(w,y)=\\max_{w'}u(Rw+y-w')+\\beta\\sum_{y'\\in Y}V(w',y')Q(y',y)$$\n",
        "\n",
        "where\n",
        "\n",
        "$$u(c)=\\frac{c^{1+\\gamma}}{1+\\gamma}$$\n",
        "\n",
        "**Details on the model assumptions and detailed derivation of the Bellman equation** comes in the `7_OptimalSavingsNumba` notebook.\n",
        "\n",
        "The main objetive of this notebook is to prove **how `jax` improve the performance** of the model solvers that we create in `7_OptimalSavingsNumba`"
      ],
      "metadata": {
        "id": "gh0p5stFlEEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model = namedtuple('Model',('beta', 'R', 'gamma', 'w_grid', 'y_grid', 'Q'))\n",
        "\n",
        "def create_consumption_model(R=1.01,                    # Gross interest rate\n",
        "                             beta=0.98,                 # Discount factor\n",
        "                             gamma=2.5,                 # CRRA parameter\n",
        "                             w_min=0.01,                # Min wealth\n",
        "                             w_max=5.0,                 # Max wealth\n",
        "                             w_size=150,                # Grid side\n",
        "                             rho=0.9,                   # Persistance of the shock\n",
        "                             nu=0.1,                    # Volatility of stochastic error (sigma)\n",
        "                             y_size=100):\n",
        "\n",
        "    w_grid = np.linspace(w_min, w_max, w_size)\n",
        "    mc = qe.tauchen(rho= rho, sigma = nu, n = y_size)\n",
        "    y_grid, Q = np.exp(mc.state_values), mc.P\n",
        "    return Model(beta=beta, R=R, gamma=gamma, w_grid=w_grid, y_grid=y_grid, Q=Q)"
      ],
      "metadata": {
        "id": "RZP0fDATmm-3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use `jax` **we have to create a `jax`-compatible version** of the consumption model.\n",
        "\n",
        "This `jax`-compatible version of the consumption model:\n",
        "\n",
        "\n",
        "*  Break up parameters into static and nonstatic components\n",
        "*   Move the arrays to a device, for example the GPU or the TPU.\n",
        "\n",
        "Moving the arrays to a device is very **important because:**\n",
        "\n",
        "\n",
        "1.   GPUs (Graphics Processing Unit) and TPUs (Tensor Processing Unit) are designed to **perform parallel calculations** much faster than CPUs (Central Processing Unit).\n",
        "2.   By moving data to the device, JAX is allowed to perform **parallel calculations** on specialized hardware, which can significantly speed up calculation performance and efficiency.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JyNQzXzgo3Dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a JAX-compatible version of the consumption model\n",
        "def create_consumption_model_jax():\n",
        "\n",
        "    model = create_consumption_model()          # Create an instance of the model\n",
        "    beta, R, gamma, w_grid, y_grid, Q = model   # Unpacked model parameters\n",
        "\n",
        "    # Break up parameters into static and nonstatic components\n",
        "    constants = beta, R, gamma\n",
        "    sizes = len(w_grid), len(y_grid)\n",
        "    arrays = w_grid, y_grid, Q\n",
        "\n",
        "    arrays = tuple(map(jax.device_put, arrays)) # Shift arrays to the device (e.g., GPU and TPU)\n",
        "\n",
        "    return constants, sizes, arrays"
      ],
      "metadata": {
        "id": "OYABl6S9oZI5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we compute the RHS of the **Bellman equation**."
      ],
      "metadata": {
        "id": "SjAGxQUXswBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def B(v, constants, sizes, arrays):\n",
        "\n",
        "    # Unpack model parameters\n",
        "    β, R, γ = constants\n",
        "    w_size, y_size = sizes\n",
        "    w_grid, y_grid, Q = arrays\n",
        "\n",
        "    # Notice that index:\n",
        "    #   - i is for current wealth\n",
        "    #   - j is for current income\n",
        "    #   - ip is for next period's wealth\n",
        "    #   - jp is for next period's income\n",
        "\n",
        "    # Compute current rewards r(w, y, wp) as an array r[i, j, ip] of three dimesions\n",
        "    w  = jnp.reshape(w_grid, (w_size, 1, 1))    # w[i]   ->  w[i, j, ip]\n",
        "    y  = jnp.reshape(y_grid, (1, y_size, 1))    # z[j]   ->  z[i, j, ip]\n",
        "    wp = jnp.reshape(w_grid, (1, 1, w_size))    # wp[ip] -> wp[i, j, ip]\n",
        "    c = R * w + y - wp                          # Current reward\n",
        "\n",
        "    # Calculate continuation rewards at all combinations of (w, y, wp)\n",
        "    v = jnp.reshape(v, (1, 1, w_size, y_size))  # v[ip, jp] -> v[i, j, ip, jp]\n",
        "    Q = jnp.reshape(Q, (1, y_size, 1, y_size))  # Q[j, jp]  -> Q[i, j, ip, jp]\n",
        "    EV = jnp.sum(v * Q, axis=3)                 # Sum over last index jp for the expected value\n",
        "\n",
        "    # Comoute the RHS of the Bellman equation if and only if c>0, otherwise set -np.inf\n",
        "    return jnp.where(c > 0, c**(1-γ)/(1-γ) + β * EV, -np.inf)"
      ],
      "metadata": {
        "id": "_IxPuSYFs1P6"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}