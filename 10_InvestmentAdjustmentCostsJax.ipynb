{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM8aLJbwaWrUBeUHPaULcX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mloyorev/Theory/blob/main/10_InvestmentAdjustmentCostsJax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install quantecon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8MwcC5s9pML",
        "outputId": "9f1b9e6d-6e17-4105-ddfd-e1787e9fd80d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: quantecon in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: numba>=0.49.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (0.58.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from quantecon) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.11.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.12)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49.0->quantecon) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->quantecon) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "b0gyKOxp9czJ"
      },
      "outputs": [],
      "source": [
        "import quantecon as qe\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in `9_OptimalInvestmentJax.ipynb` we use 64 bit floats."
      ],
      "metadata": {
        "id": "0Gc-AfSU90O6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jax.config.update(\"jax_enable_x64\", True)"
      ],
      "metadata": {
        "id": "Bfxux9vJ99G_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in the previous notebook, we define the `succesive_approx` function:"
      ],
      "metadata": {
        "id": "gJHv6H1v-Rqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def successive_approx(T,                     # Operator (callable)\n",
        "                      x_0,                   # Initial condition\n",
        "                      tolerance=1e-6,        # Error tolerance\n",
        "                      max_iter=10000,        # Max iteration bound\n",
        "                      print_step=25,         # Print at multiples\n",
        "                      verbose=False):\n",
        "\n",
        "    x = x_0\n",
        "    error = tolerance + 1\n",
        "    k = 1\n",
        "\n",
        "    while error > tolerance and k <= max_iter:\n",
        "        x_new = T(x)\n",
        "        error = np.max(np.abs(x_new - x))\n",
        "        if verbose and k % print_step == 0:\n",
        "            print(f\"Completed iteration {k} with error {error}.\")\n",
        "        x = x_new\n",
        "        k += 1\n",
        "\n",
        "    if error > tolerance:\n",
        "        print(f\"Warning: Iteration hit upper bound {max_iter}.\")\n",
        "\n",
        "    elif verbose:\n",
        "        print(f\"Terminated successfully in {k} iterations.\")\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "tx6IZu9L-ak7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Investment with Adjustment Costs with Google Jax**\n",
        "\n",
        "As we have already mentioned, the **Bellman equation** of this model is given by\n",
        "\n",
        "$$V(y,z)=\\max_{y'}r(y,z,y')+β∑_{z'}V(y',z')Q(z,z')$$\n",
        "\n",
        "**Details** on the assumptions of the model and derivation of the Bellman equation comes in the `8_InvestmentAdjustmentCostNumba.ipynb`.\n",
        "\n",
        "The **main pourpose of this notebook** is to prove how `jax` improve the performance of the model solvers created in `8_InvestmentAdjustmentCostNumba.ipynb`."
      ],
      "metadata": {
        "id": "YDwnh-OU-jet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model = namedtuple(\"Model\", (\"beta\", \"a_0\", \"a_1\", \"gamma\", \"c\",\"y_size\", \"z_size\", \"y_grid\", \"z_grid\", \"Q\"))\n",
        "\n",
        "def create_investment_model(\n",
        "        r=0.01,                              # Interest rate\n",
        "        a_0=10.0, a_1=1.0,                   # Demand parameters\n",
        "        gamma=25.0, c=1.0,                   # Adjustment and unit cost\n",
        "        y_min=0.0, y_max=20.0, y_size=100,   # Grid for output\n",
        "        rho=0.9, nu=1.0,                     # AR(1) parameters\n",
        "        z_size=150):                         # Grid size for shock\n",
        "\n",
        "    beta = 1/(1+r)\n",
        "    y_grid = np.linspace(y_min, y_max, y_size)\n",
        "    mc = qe.tauchen(rho=rho, sigma=nu, n=z_size)\n",
        "    z_grid, Q = mc.state_values, mc.P\n",
        "\n",
        "    model = Model(beta=beta, a_0=a_0, a_1=a_1, gamma=gamma, c=c, y_size=y_size, z_size=z_size, y_grid=y_grid, z_grid=z_grid, Q=Q)\n",
        "    return model"
      ],
      "metadata": {
        "id": "kVZm2VPZ-p6Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we modify the model to make it easier to pass to `jax` functions."
      ],
      "metadata": {
        "id": "B-uvvO8HAA1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_investment_model_jax():\n",
        "    model = create_investment_model()\n",
        "    beta, a_0, a_1, gamma, c, y_size, z_size, y_grid, z_grid, Q = model\n",
        "\n",
        "    # Break up parameters into static and nonstatic components\n",
        "    constants = beta, a_0, a_1, gamma, c\n",
        "    sizes = y_size, z_size\n",
        "    arrays = y_grid, z_grid, Q\n",
        "\n",
        "    # Shift arrays to the device (e.g., GPU)\n",
        "    arrays = tuple(map(jax.device_put, arrays))\n",
        "    return constants, sizes, arrays"
      ],
      "metadata": {
        "id": "C2Ai2V0CALxI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we create a vectorized version of the RHS of the **Bellman equation** (before maximization), which is a 3D array represented by\n",
        "\n",
        "$$B(y,z,y')=r(y,z,y')+\\beta\\sum_{z'}v(y',z')Q(y',y)$$"
      ],
      "metadata": {
        "id": "-nCCGlE7Aa4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def B(v, constants, sizes, arrays):\n",
        "    # Unpack\n",
        "    beta, a_0, a_1, gamma, c = constants\n",
        "    y_size, z_size = sizes\n",
        "    y_grid, z_grid, Q = arrays\n",
        "\n",
        "    # Compute current rewards r(y, z, yp) as array r[i, j, ip]\n",
        "    y  = jnp.reshape(y_grid, (y_size, 1, 1))           # y[i]   ->  y[i, j, ip]\n",
        "    z  = jnp.reshape(z_grid, (1, z_size, 1))           # z[j]   ->  z[i, j, ip]\n",
        "    yp = jnp.reshape(y_grid, (1, 1, y_size))           # yp[ip] -> yp[i, j, ip]\n",
        "    r = (a_0 - a_1 * y + z - c) * y - gamma * (yp - y)**2\n",
        "\n",
        "    # Calculate continuation rewards at all combinations of (y, z, yp)\n",
        "    v = jnp.reshape(v, (1, 1, y_size, z_size))  # v[ip, jp] -> v[i, j, ip, jp]\n",
        "    Q = jnp.reshape(Q, (1, z_size, 1, z_size))  # Q[j, jp]  -> Q[i, j, ip, jp]\n",
        "    EV = jnp.sum(v * Q, axis=3)                 # sum over last index jp\n",
        "\n",
        "    # Compute the right-hand side of the Bellman equation\n",
        "    return r + beta * EV"
      ],
      "metadata": {
        "id": "KDfexen6A-ly"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the proofs contained in the `8_InvestmentAdjustmentCosts.ipynb` notebook, we know that the model satisfies the Blackwell sufficiency conditions to be a contraction mapping and, therefore, **has a unique solution**.\n",
        "\n",
        "As in the previous notebook, we are going to use the **following algorithms** to solve the model:\n",
        "\n",
        "\n",
        "*   Value Function Iteration (VFI).\n",
        "*   Howard Policy Iteration (HPI).\n",
        "*   Optimistic Policy Iteration (OPI).\n",
        "\n",
        "Now we define the necessary operators for each algorithm\n",
        "\n"
      ],
      "metadata": {
        "id": "NzxcOiv4BYB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----COMPUTE CURRENT REWARD-----\n",
        "def compute_r_sigma(sigma, constants, sizes, arrays):\n",
        "  # Unpack model\n",
        "    beta, a_0, a_1, gamma, c = constants\n",
        "    y_size, z_size = sizes\n",
        "    y_grid, z_grid, Q = arrays\n",
        "\n",
        "  # Compute r_σ[i, j]\n",
        "    y = jnp.reshape(y_grid, (y_size, 1))  # y[i]   ->  y[i, j]\n",
        "    z = jnp.reshape(z_grid, (1, z_size))  # z[j]   ->  z[i, j]\n",
        "    yp = y_grid[sigma]                    # Selection of values ​​from a grid y_grid using indices contained in an array called sigma.\n",
        "\n",
        "    r_sigma = (a_0 - a_1 * y + z - c) * y - gamma * (yp - y) ** 2      # Compute current reward\n",
        "\n",
        "    return r_sigma\n",
        "\n",
        "# -----BELLMAN OPERATOR-----\n",
        "def T(v, constants, sizes, arrays):\n",
        "    return jnp.max(B(v, constants, sizes, arrays), axis=2)  #   The result of the B function is passed through the jnp.max function with axis=2. This means that\n",
        "                                                            # the maximum value is computed along the third axis, which corresponds to the 'wp' axis. This operation\n",
        "                                                            # calculates the maximum value for each combination of (w,y). The result of the T function is a 2D matrix\n",
        "\n",
        "# -----GET GREEDY-----\n",
        "def get_greedy(v, constants, sizes, arrays):\n",
        "    return jnp.argmax(B(v, constants, sizes, arrays), axis=2) #   The result of the B function is then passed through the jnp.argmax function with axis=2. This means that the index\n",
        "                                                              # of the maximum value is computed along the third axis, which corresponds to the 'wp' axis. This operation finds the\n",
        "                                                              # index of the action that maximizes RHS of the Bellman equation for each combination (w,y).\n",
        "\n",
        "# -----POLICY OPERATOR-----\n",
        "def T_sigma(v, sigma, constants, sizes, arrays):\n",
        "    # Unpack model\n",
        "    beta, a_0, a_1, gamma, c = constants\n",
        "    y_size, z_size = sizes\n",
        "    y_grid, z_grid, Q = arrays\n",
        "\n",
        "    r_sigma = compute_r_sigma(sigma, constants, sizes, arrays)  # Compute current reward\n",
        "\n",
        "    # Compute the array v[σ[i, j], jp]\n",
        "    zp_idx = jnp.arange(z_size)\n",
        "    zp_idx = jnp.reshape(zp_idx, (1, 1, z_size))\n",
        "    sigma = jnp.reshape(sigma, (y_size, z_size, 1))\n",
        "    V = v[sigma, zp_idx]\n",
        "\n",
        "    # Convert Q[j, jp] to Q[i, j, jp]\n",
        "    Q = jnp.reshape(Q, (1, z_size, z_size))\n",
        "\n",
        "    return r_sigma + beta * np.sum(V * Q, axis=2)"
      ],
      "metadata": {
        "id": "2GAcC148Dw49"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One more time, we define the **functions needed to compute the value** $v_{\\sigma}$ of follow a particular policy $\\sigma$."
      ],
      "metadata": {
        "id": "-EZSENaIGLwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L_sigma(v, sigma, constants, sizes, arrays):\n",
        "    # Unpack model\n",
        "    beta, a_0, a_1, gamma, c = constants\n",
        "    y_size, z_size = sizes\n",
        "    y_grid, z_grid, Q = arrays\n",
        "\n",
        "    # Set up the array v[σ[i, j], jp]\n",
        "    zp_idx = jnp.arange(z_size)                     # Create one-dimensional arrays ranging from 0 to y_size-1\n",
        "    zp_idx = jnp.reshape(zp_idx, (1, 1, z_size))    # Reshape zp_idx as an array of three dimensiones (i,j,ip)\n",
        "    sigma = jnp.reshape(sigma, (y_size, z_size, 1)) # Reshape sigma as an array of three dimensiones (i,j,ip)\n",
        "    V = v[sigma, zp_idx]                            # Evaluate v(w',y')\n",
        "\n",
        "    # Expand Q[j, jp] to Q[i, j, jp]\n",
        "    Q = jnp.reshape(Q, (1, y_size, y_size))\n",
        "\n",
        "    # Compute and return v[i, j] - β Σ_jp v[σ[i, j], jp] * Q[j, jp]\n",
        "    return v - beta * np.sum(V * Q, axis=2)\n",
        "\n",
        "def get_value(sigma, constants, sizes, arrays):\n",
        "    # Unpack\n",
        "    beta, a_0, a_1, gamma, c = constants\n",
        "    y_size, z_size = sizes\n",
        "    y_grid, z_grid, Q = arrays\n",
        "\n",
        "    r_sigma = compute_r_sigma(sigma, constants, sizes, arrays)               # Computes current reward\n",
        "\n",
        "    # Reduce R_σ to a function in v\n",
        "    partial_R_sigma = lambda v: L_sigma(v, sigma, constants, sizes, arrays)  # Defines a function 'partial_R_sigma' that takes v as an argument\n",
        "\n",
        "    return jax.scipy.sparse.linalg.bicgstab(partial_R_sigma, r_sigma)[0]"
      ],
      "metadata": {
        "id": "JbK2AVYdGbaJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we build the `JIT` compiled versions of the previous functions. For a more detailed explanation of how does `static_argnums` works, check `9_OptimalInvestment.ipynb`."
      ],
      "metadata": {
        "id": "TUTGiX7_HHtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = jax.jit(B, static_argnums=(2,))\n",
        "compute_r_σ = jax.jit(compute_r_sigma, static_argnums=(2,))\n",
        "T = jax.jit(T, static_argnums=(2,))\n",
        "get_greedy = jax.jit(get_greedy, static_argnums=(2,))\n",
        "get_value = jax.jit(get_value, static_argnums=(2,))\n",
        "\n",
        "T_sigma = jax.jit(T_sigma, static_argnums=(3,))\n",
        "L_sigma = jax.jit(L_sigma, static_argnums=(3,))"
      ],
      "metadata": {
        "id": "J9WZ1tfpHV0h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we introduce functions for each algorithm (VFI, OPI and HPI)."
      ],
      "metadata": {
        "id": "HBbdZulQI9Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Value Function Iteration\n",
        "def value_iteration(model, tol=1e-5):\n",
        "    constants, sizes, arrays = model\n",
        "    vz = jnp.zeros(sizes)\n",
        "\n",
        "    v_star = successive_approx(lambda v: T(v, constants, sizes, arrays), vz, tolerance=tol)\n",
        "    return get_greedy(v_star, constants, sizes, arrays)\n",
        "\n",
        "# Howard Policy Iteration\n",
        "def policy_iteration(model, maxiter=250):\n",
        "    constants, sizes, arrays = model\n",
        "    vz = jnp.zeros(sizes)\n",
        "    sigma = jnp.zeros(sizes, dtype=int)\n",
        "    i, error = 0, 1.0\n",
        "    while error > 0 and i < maxiter:\n",
        "        v_sigma = get_value(sigma, constants, sizes, arrays)\n",
        "        sigma_new = get_greedy(v_sigma, constants, sizes, arrays)\n",
        "        error = jnp.max(jnp.abs(sigma_new - sigma))\n",
        "        sigma = sigma_new\n",
        "        i = i + 1\n",
        "        print(f\"Concluded loop {i} with error {error}.\")\n",
        "    return sigma\n",
        "\n",
        "# Optimistic Policy Iteration\n",
        "def optimistic_policy_iteration(model, tol=1e-5, m=10):\n",
        "    constants, sizes, arrays = model\n",
        "    v = jnp.zeros(sizes)\n",
        "    error = tol + 1\n",
        "    while error > tol:\n",
        "        last_v = v\n",
        "        sigma = get_greedy(v, constants, sizes, arrays)\n",
        "        for _ in range(m):\n",
        "            v = T_sigma(v, sigma, constants, sizes, arrays)\n",
        "        error = jnp.max(jnp.abs(v - last_v))\n",
        "    return get_greedy(v, constants, sizes, arrays)"
      ],
      "metadata": {
        "id": "pNcTSlHFJHpd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, here's a **test** of each solver."
      ],
      "metadata": {
        "id": "uyHdfGjsJbO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_investment_model_jax()"
      ],
      "metadata": {
        "id": "aTfye7jgKAHT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting VFI.\")\n",
        "qe.tic()\n",
        "out = value_iteration(model)\n",
        "elapsed = qe.toc()\n",
        "print(out)\n",
        "print(f\"VFI completed in {elapsed} seconds.\")"
      ],
      "metadata": {
        "id": "Cd7-JEuyhYap",
        "outputId": "e7051a74-8584-4bc4-cd7b-12526d2d7637",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting VFI.\n",
            "TOC: Elapsed: 0:00:17.90\n",
            "[[ 2  2  2 ...  6  6  6]\n",
            " [ 3  3  3 ...  7  7  7]\n",
            " [ 4  4  4 ...  7  7  7]\n",
            " ...\n",
            " [82 82 82 ... 86 86 86]\n",
            " [83 83 83 ... 86 86 86]\n",
            " [84 84 84 ... 87 87 87]]\n",
            "VFI completed in 17.905871152877808 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting OPI.\")\n",
        "qe.tic()\n",
        "out = optimistic_policy_iteration(model, m=100)\n",
        "elapsed = qe.toc()\n",
        "print(out)\n",
        "print(f\"OPI completed in {elapsed} seconds.\")"
      ],
      "metadata": {
        "id": "KSGfjtTVhdDM",
        "outputId": "2e42e7e1-288d-4947-c376-9db8f1a39262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting OPI.\n",
            "TOC: Elapsed: 0:00:8.15\n",
            "[[ 2  2  2 ...  6  6  6]\n",
            " [ 3  3  3 ...  7  7  7]\n",
            " [ 4  4  4 ...  7  7  7]\n",
            " ...\n",
            " [82 82 82 ... 86 86 86]\n",
            " [83 83 83 ... 86 86 86]\n",
            " [84 84 84 ... 87 87 87]]\n",
            "OPI completed in 8.158338069915771 seconds.\n"
          ]
        }
      ]
    }
  ]
}