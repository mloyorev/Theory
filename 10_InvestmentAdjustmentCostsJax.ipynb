{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPspKSU/KCmK7PqBd95PGZe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mloyorev/Theory/blob/main/10_InvestmentAdjustmentCostsJax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install quantecon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8MwcC5s9pML",
        "outputId": "61159f06-f7df-4008-ec9a-a8af61098366"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: quantecon in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: numba>=0.49.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (0.58.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from quantecon) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.11.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from quantecon) (1.12)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49.0->quantecon) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->quantecon) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->quantecon) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b0gyKOxp9czJ"
      },
      "outputs": [],
      "source": [
        "import quantecon as qe\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in `9_OptimalInvestmentJax.ipynb` we use 64 bit floats."
      ],
      "metadata": {
        "id": "0Gc-AfSU90O6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jax.config.update(\"jax_enable_x64\", True)"
      ],
      "metadata": {
        "id": "Bfxux9vJ99G_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As in the previous notebook, we define the `succesive_approx` function:"
      ],
      "metadata": {
        "id": "gJHv6H1v-Rqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def successive_approx(T,                     # Operator (callable)\n",
        "                      x_0,                   # Initial condition\n",
        "                      tolerance=1e-6,        # Error tolerance\n",
        "                      max_iter=10000,        # Max iteration bound\n",
        "                      print_step=25,         # Print at multiples\n",
        "                      verbose=False):\n",
        "\n",
        "    x = x_0\n",
        "    error = tolerance + 1\n",
        "    k = 1\n",
        "\n",
        "    while error > tolerance and k <= max_iter:\n",
        "        x_new = T(x)\n",
        "        error = np.max(np.abs(x_new - x))\n",
        "        if verbose and k % print_step == 0:\n",
        "            print(f\"Completed iteration {k} with error {error}.\")\n",
        "        x = x_new\n",
        "        k += 1\n",
        "\n",
        "    if error > tolerance:\n",
        "        print(f\"Warning: Iteration hit upper bound {max_iter}.\")\n",
        "\n",
        "    elif verbose:\n",
        "        print(f\"Terminated successfully in {k} iterations.\")\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "tx6IZu9L-ak7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Investment with Adjustment Costs with Google Jax**\n",
        "\n",
        "As we have already mentioned, the **Bellman equation** of this model is given by\n",
        "\n",
        "$$V(y,z)=\\max_{y'}r(y,z,y')+β∑_{z'}V(y',z')Q(z,z')$$\n",
        "\n",
        "**Details** on the assumptions of the model and derivation of the Bellman equation comes in the `8_InvestmentAdjustmentCostNumba.ipynb`.\n",
        "\n",
        "The **main pourpose of this notebook** is to prove how `jax` improve the performance of the model solvers created in `8_InvestmentAdjustmentCostNumba.ipynb`."
      ],
      "metadata": {
        "id": "YDwnh-OU-jet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model = namedtuple(\"Model\", (\"beta\", \"a_0\", \"a_1\", \"gamma\", \"c\",\"y_size\", \"z_size\", \"y_grid\", \"z_grid\", \"Q\"))\n",
        "\n",
        "def create_investment_model(\n",
        "        r=0.01,                              # Interest rate\n",
        "        a_0=10.0, a_1=1.0,                   # Demand parameters\n",
        "        gamma=25.0, c=1.0,                   # Adjustment and unit cost\n",
        "        y_min=0.0, y_max=20.0, y_size=100,   # Grid for output\n",
        "        rho=0.9, nu=1.0,                     # AR(1) parameters\n",
        "        z_size=150):                         # Grid size for shock\n",
        "\n",
        "    β = 1/(1+r)\n",
        "    y_grid = np.linspace(y_min, y_max, y_size)\n",
        "    mc = qe.tauchen(ρ, ν, n=z_size)\n",
        "    z_grid, Q = mc.state_values, mc.P\n",
        "\n",
        "    model = Model(β=β, a_0=a_0, a_1=a_1, γ=γ, c=c, y_size=y_size, z_size=z_size, y_grid=y_grid, z_grid=z_grid, Q=Q)\n",
        "    return model"
      ],
      "metadata": {
        "id": "kVZm2VPZ-p6Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we modify the model to make it easier to pass to `jax` functions."
      ],
      "metadata": {
        "id": "B-uvvO8HAA1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_investment_model_jax():\n",
        "    model = create_investment_model()\n",
        "    beta, a_0, a_1, gamma, c, y_size, z_size, y_grid, z_grid, Q = model\n",
        "\n",
        "    # Break up parameters into static and nonstatic components\n",
        "    constants = beta, a_0, a_1, gamma, c\n",
        "    sizes = y_size, z_size\n",
        "    arrays = y_grid, z_grid, Q\n",
        "\n",
        "    # Shift arrays to the device (e.g., GPU)\n",
        "    arrays = tuple(map(jax.device_put, arrays))\n",
        "    return constants, sizes, arrays"
      ],
      "metadata": {
        "id": "C2Ai2V0CALxI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we create a vectorized version of the RHS of the **Bellman equation** (before maximization), which is a 3D array represented by\n",
        "\n",
        "$$B(y,z,y')=r(y,z,y')+\\beta\\sum_{z'}v(y',z')Q(y',y)$$"
      ],
      "metadata": {
        "id": "-nCCGlE7Aa4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def B(v, constants, sizes, arrays):\n",
        "    # Unpack\n",
        "    beta, a_0, a_1, gamma, c = constants\n",
        "    y_size, z_size = sizes\n",
        "    y_grid, z_grid, Q = arrays\n",
        "\n",
        "    # Compute current rewards r(y, z, yp) as array r[i, j, ip]\n",
        "    y  = jnp.reshape(y_grid, (y_size, 1, 1))           # y[i]   ->  y[i, j, ip]\n",
        "    z  = jnp.reshape(z_grid, (1, z_size, 1))           # z[j]   ->  z[i, j, ip]\n",
        "    yp = jnp.reshape(y_grid, (1, 1, y_size))           # yp[ip] -> yp[i, j, ip]\n",
        "    r = (a_0 - a_1 * y + z - c) * y - gamma * (yp - y)**2\n",
        "\n",
        "    # Calculate continuation rewards at all combinations of (y, z, yp)\n",
        "    v = jnp.reshape(v, (1, 1, y_size, z_size))  # v[ip, jp] -> v[i, j, ip, jp]\n",
        "    Q = jnp.reshape(Q, (1, z_size, 1, z_size))  # Q[j, jp]  -> Q[i, j, ip, jp]\n",
        "    EV = jnp.sum(v * Q, axis=3)                 # sum over last index jp\n",
        "\n",
        "    # Compute the right-hand side of the Bellman equation\n",
        "    return r + beta * EV"
      ],
      "metadata": {
        "id": "KDfexen6A-ly"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the proofs contained in the `8_InvestmentAdjustmentCosts.ipynb` notebook, we know that the model satisfies the Blackwell sufficiency conditions to be a contraction mapping and, therefore, **has a unique solution**.\n",
        "\n",
        "As in the previous notebook, we are going to use the **following algorithms** to solve the model:\n",
        "\n",
        "\n",
        "*   Value Function Iteration (VFI).\n",
        "*   Howard Policy Iteration (HPI).\n",
        "*   Optimistic Policy Iteration (OPI).\n",
        "\n",
        "Now we define the necessary operators for each algorithm\n",
        "\n"
      ],
      "metadata": {
        "id": "NzxcOiv4BYB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----COMPUTE CURRENT REWARD-----\n",
        "def compute_r_sigma(sigma, constants, sizes, arrays):\n",
        "  # Unpack model\n",
        "    beta, a_0, a_1, gamma, c = constants\n",
        "    y_size, z_size = sizes\n",
        "    y_grid, z_grid, Q = arrays\n",
        "\n",
        "  # Compute r_σ[i, j]\n",
        "    y = jnp.reshape(y_grid, (y_size, 1))  # y[i]   ->  y[i, j]\n",
        "    z = jnp.reshape(z_grid, (1, z_size))  # z[j]   ->  z[i, j]\n",
        "    yp = y_grid[sigma]                    # Selection of values ​​from a grid y_grid using indices contained in an array called sigma.\n",
        "\n",
        "    r_sigma = (a_0 - a_1 * y + z - c) * y - gamma * (yp - y) ** 2      # Compute current reward\n",
        "\n",
        "    return r_sigma\n",
        "\n",
        "# -----BELLMAN OPERATOR-----\n",
        "def T(v, constants, sizes, arrays):\n",
        "    return jnp.max(B(v, constants, sizes, arrays), axis=2)  #   The result of the B function is passed through the jnp.max function with axis=2. This means that\n",
        "                                                            # the maximum value is computed along the third axis, which corresponds to the 'wp' axis. This operation\n",
        "                                                            # calculates the maximum value for each combination of (w,y). The result of the T function is a 2D matrix\n",
        "\n",
        "# -----GET GREEDY-----\n",
        "def get_greedy(v, constants, sizes, arrays):\n",
        "    return jnp.argmax(B(v, constants, sizes, arrays), axis=2) #   The result of the B function is then passed through the jnp.argmax function with axis=2. This means that the index\n",
        "                                                              # of the maximum value is computed along the third axis, which corresponds to the 'wp' axis. This operation finds the\n",
        "                                                              # index of the action that maximizes RHS of the Bellman equation for each combination (w,y).\n",
        "\n",
        "# -----POLICY OPERATOR-----\n",
        "def T_sigma(v, sigma, constants, sizes, arrays):\n",
        "    # Unpack model\n",
        "    beta, a_0, a_1, gamma, c = constants\n",
        "    y_size, z_size = sizes\n",
        "    y_grid, z_grid, Q = arrays\n",
        "\n",
        "    r_sigma = compute_r_sigma(sigma, constants, sizes, arrays)  # Compute current reward\n",
        "\n",
        "    # Compute the array v[σ[i, j], jp]\n",
        "    zp_idx = jnp.arange(z_size)\n",
        "    zp_idx = jnp.reshape(zp_idx, (1, 1, z_size))\n",
        "    sigma = jnp.reshape(sigma, (y_size, z_size, 1))\n",
        "    V = v[sigma, zp_idx]\n",
        "\n",
        "    # Convert Q[j, jp] to Q[i, j, jp]\n",
        "    Q = jnp.reshape(Q, (1, z_size, z_size))\n",
        "\n",
        "    return r_sigma + beta * np.sum(V * Q, axis=2)"
      ],
      "metadata": {
        "id": "2GAcC148Dw49"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}